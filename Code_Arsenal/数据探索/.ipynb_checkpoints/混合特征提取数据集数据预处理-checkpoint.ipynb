{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn import cluster\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读入数据存放至data_df的数据结构中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_path = '/home/390672/WORKSHOP/noise_analysis/清洗后的特征数据/混合特征提取/6特征集全齐/normalfulldata_source.xlsx'\n",
    "\n",
    "data_df = pd.read_excel(data_path)\n",
    "data_df.index = range(data_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一. 数据预处理工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/390672/anaconda3/envs/Zootopia3_4/lib/python3.4/site-packages/pandas/core/indexing.py:132: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# 如果某列属性整列都是nan值，直接将该列删除\n",
    "data_df = data_df.dropna(how=\"all\",axis=1)\n",
    "\n",
    "columns = data_df.columns\n",
    "# 对inf的数据进行填充\n",
    "for each in columns:\n",
    "\n",
    "    temp = data_df.loc[:,each]\n",
    "    temp.loc[temp == np.inf] = np.nan\n",
    "    data_df.ix[:,each] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fullfill_nan(data_df,names):\n",
    "    # input:\n",
    "    # data_df - 数据表DataFrame结构，一张完整的数据表\n",
    "    # names - 特征集的名称，list\n",
    "    # output：\n",
    "    # 将特征集中所有有效的特征（不含nan值）进行聚类，在根据聚类情况将值赋给nan的元素\n",
    "    \n",
    "    # 输入的names对应的特征集的所有数据\n",
    "    cluster_data = data_df[names]\n",
    "    # cluster_data2 - 只包含全部有效值数据表（完全没有nan值）\n",
    "    cluster_data2 = cluster_data\n",
    "    for each in names:\n",
    "        if not(any(cluster_data2[each].isnull())):\n",
    "            # 没有非空的值，全部是有效值\n",
    "            pass\n",
    "        else:\n",
    "            cluster_data2 = cluster_data2.drop(each,axis=1)\n",
    "\n",
    "    # col_valid 所有有效的特征的列名\n",
    "    # col_rest 所有含无效值的特征的列名\n",
    "    col_full = cluster_data.columns\n",
    "    col_valid = cluster_data2.columns.values.tolist()\n",
    "    col_rest = col_full.drop(col_valid).values.tolist()\n",
    "\n",
    "    for each in col_rest:\n",
    "        nan_ind = cluster_data2.index[data_df[each].isnull()]\n",
    "        notnan_ind = cluster_data2.index[data_df[each].notnull()]\n",
    "\n",
    "        train_data = cluster_data2.ix[notnan_ind,:] # 将nan的样本点剔除掉的样本库\n",
    "        nan_data = cluster_data2.ix[nan_ind,:] # 只有nan的样本点的样本库\n",
    "        try:\n",
    "            ###################################################################################\n",
    "            ################  对读入的数据进行聚类，再根据聚类结果填充空值 ####################\n",
    "            ###################################################################################\n",
    "            # 数据预处理（无量纲minmax）\n",
    "            min_max_scaler = preprocessing.MinMaxScaler()\n",
    "            X_train_minmax = min_max_scaler.fit_transform(train_data)\n",
    "            X_nan_minmax = min_max_scaler.fit_transform(nan_data)\n",
    "            # 进行聚类 K-Means\n",
    "            km = cluster.KMeans(n_clusters=2).fit(X_train_minmax)\n",
    "            labels = km.labels_\n",
    "    \n",
    "            notnan_ind_0 = train_data[labels == 0].index\n",
    "            notnan_ind_1 = train_data[labels == 1].index\n",
    "            mean_0 = data_df.ix[notnan_ind_0,each].mean()\n",
    "            mean_1 = data_df.ix[notnan_ind_1,each].mean()\n",
    "    \n",
    "            nan_labels = km.predict(X_nan_minmax)\n",
    "    \n",
    "            nan_ind_0 = nan_data[nan_labels == 0].index\n",
    "            nan_ind_1 = nan_data[nan_labels == 1].index\n",
    "    \n",
    "            data_df.loc[nan_ind_0,each] = mean_0\n",
    "            data_df.loc[nan_ind_1,each] = mean_1\n",
    "        except BaseException as e:\n",
    "            print(each + '缺失值填充失败，出现如下错误：\\n',e)\n",
    "            # 因为每个属性都有缺失值，没办法进行聚类计算补充\n",
    "            nan_ind = cluster_data2.index[data_df[each].isnull()]\n",
    "            notnan_ind = cluster_data2.index[data_df[each].notnull()]\n",
    "            mean = data_df.ix[notnan_ind,each].mean()\n",
    "            data_df.loc[nan_ind,each] = mean\n",
    "                    \n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CL3_Bandpass_2_p1缺失值填充失败，出现如下错误：\n",
      " Found array with 0 feature(s) (shape=(340, 0)) while a minimum of 1 is required by MinMaxScaler.\n",
      "CL3_Bandpass_2_p2缺失值填充失败，出现如下错误：\n",
      " Found array with 0 feature(s) (shape=(340, 0)) while a minimum of 1 is required by MinMaxScaler.\n",
      "CL3_Bandpass_2_p3缺失值填充失败，出现如下错误：\n",
      " Found array with 0 feature(s) (shape=(340, 0)) while a minimum of 1 is required by MinMaxScaler.\n",
      "CL3_Bandpass_2_p4缺失值填充失败，出现如下错误：\n",
      " Found array with 0 feature(s) (shape=(340, 0)) while a minimum of 1 is required by MinMaxScaler.\n",
      "CL3_Bandpass_2_p5缺失值填充失败，出现如下错误：\n",
      " Found array with 0 feature(s) (shape=(340, 0)) while a minimum of 1 is required by MinMaxScaler.\n",
      "CL4_Bandpass_2_p12缺失值填充失败，出现如下错误：\n",
      " Found array with 0 feature(s) (shape=(340, 0)) while a minimum of 1 is required by MinMaxScaler.\n",
      "CL4_Bandpass_2_p13缺失值填充失败，出现如下错误：\n",
      " Found array with 0 feature(s) (shape=(340, 0)) while a minimum of 1 is required by MinMaxScaler.\n",
      "0.4114058017730713\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "time_a = time.time()\n",
    "list1 = dict() #缺失数量大于20 %\n",
    "list2 = dict() #缺失数量少于等于20%\n",
    "\n",
    "# 对于某列含有部分缺失值的数据进行处理\n",
    "for each in columns:\n",
    "    temp = data_df.loc[:,each].notnull() # temp为True代表是非空的元素\n",
    "    \n",
    "    if all(temp):\n",
    "        # 没有非空的值，全部是有效值\n",
    "        pass\n",
    "    elif len(temp[temp==False])/len(temp) > 0.2:\n",
    "        # 1. 缺失数量大于20 %\n",
    "        #  如果含有inf值的元素，使用nan值来替换\n",
    "        #    用KNN找出最近邻的值作填充\n",
    "        # 获取该列所处的特征集名称\n",
    "        m = re.findall('.*_p',each) # each:'cl1_p2'; m :'cl1_p'\n",
    "        group_name = m[0][:-2]  # group_name: 'cl1'\n",
    "        list1[group_name] = [each_b for each_b in pd.Series(columns.values) if group_name in each_b]\n",
    "        \n",
    "    else:\n",
    "        # 2. 缺失数量少于等于20%\n",
    "        #    用平均值\n",
    "        # 找出 缺失数量大于20 % 的列\n",
    "        # 获取该列所处的特征集名称\n",
    "        m = re.findall('.*_p',each) # each:'cl1_p2'; m :'cl1_p'\n",
    "        group_name = m[0][:-2]  # group_name: 'cl1'\n",
    "        list2[group_name] = [each_b for each_b in pd.Series(columns.values) if group_name in each_b]\n",
    "        \n",
    "for each in list1.keys():\n",
    "    data_df = fullfill_nan(data_df=data_df, names=list1[each])\n",
    "\n",
    "\n",
    "print(time.time() - time_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "将剩余的没处理好的nan值数据填充完整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for each in list2.keys():\n",
    "    columns = list2[each]\n",
    "    for each_b in columns:\n",
    "        _ = data_df[each_b].fillna(data_df[each_b].mean(),inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绘出某一数据集的数据分布图\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib notebook\n",
    "matplotlib.matplotlib_fname()\n",
    "columns = data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "# 找出目标的数据集\n",
    "target_col=list()\n",
    "for each in columns:\n",
    "    if 'cl1' in each:\n",
    "        target_col.append(each)\n",
    "    else:\n",
    "        pass\n",
    "row = 3;col = len(target_col)//row\n",
    "for ii in range(len(target_col)):\n",
    "    ax = fig.add_subplot(col,row,ii+1)\n",
    "    array = data_df[target_col[ii]]\n",
    "    ax_hist=ax.hist(array,bins=100,color='y',alpha=1,linewidth=0.1)\n",
    "    ax.set_title(target_col[ii])\n",
    "plt.subplots_adjust(hspace = 2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对标签进行哑变量编码化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec = DictVectorizer()\n",
    "data_label = data_df['machine_type']\n",
    "# lists of feature-value \n",
    "feature_value = list()\n",
    "for each in data_label:\n",
    "    feature_value.append({'machine_type':each})\n",
    "data_label = vec.fit_transform(feature_value).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 先对数据进行无量纲化的预处理\n",
    "mms = preprocessing.MinMaxScaler()\n",
    "data_df_pre = mms.fit_transform(data_df[data_df.columns.values[:-1]])\n",
    "data_df_pre = pd.DataFrame(data_df_pre,columns=data_df.columns.values[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "异常点检测剔除(outlier的点太多了，不做这一步)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.covariance import EmpiricalCovariance, MinCovDet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 用箱线图原理得出离群点\n",
    "# 建立一个dataframe来存放箱线图的数据\n",
    "temp = data_df_pre['CL3_Bandpass_0_p1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_df_pre_box = pd.DataFrame(columns=data_df_pre.columns,index=['QL','QU','IQR','QL-1.5IQR','QU+1.5IQR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for each in data_df_pre.columns:\n",
    "    data_df_pre_box.ix['QL',each] = data_df_pre[each].quantile(0.25)\n",
    "    data_df_pre_box.ix['QU',each] = data_df_pre[each].quantile(0.75)\n",
    "    data_df_pre_box.ix['IQR',each] = data_df_pre_box.ix['QU',each] - data_df_pre_box.ix['QL',each]\n",
    "    data_df_pre_box.ix['QL-1.5IQR',each] = data_df_pre_box.ix['QL',each] - 1.5 * data_df_pre_box.ix['IQR',each]\n",
    "    data_df_pre_box.ix['QU+1.5IQR',each] = data_df_pre_box.ix['QU',each] + 1.5 * data_df_pre_box.ix['IQR',each]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先进行特征选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取数据样本集\n",
    "data_source = data_df[data_df.columns.values[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 将string型的样本标签改成数值型的向量\n",
    "y = np.zeros([len(data_label),1])\n",
    "y[data_label[:,0]== 1] = 1\n",
    "y[data_label[:,1]== 1] = 2 # 0正常，1异常，2整改后正常\n",
    "y = y.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 【败】\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFECV\n",
    "svc = SVC(kernel=\"linear\")\n",
    "rfecv = RFECV(estimator=svc, step=1, cv=3,\n",
    "              scoring='accuracy')\n",
    "rfecv.fit(data_source,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 随机森林的方法\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "forest = ExtraTreesClassifier(n_estimators = 250, random_state=0)\n",
    "forest.fit(data_df_pre,y)\n",
    "importances = forest.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'DataFramea'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-b97050d536a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpercentile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mleft_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_df_pre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfeature_importance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFramea\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mleft_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'DataFramea'"
     ]
    }
   ],
   "source": [
    "percentile = 0.5\n",
    "left_num = np.around(data_df_pre.shape[1]*percentile) # 剩下排名在前百分之percentile的数\n",
    "feature_importance = pd.DataFrame(data_df_pre[indices[:left_num]],columns=data_df_pre.columns[indices[:left_num]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果是归一化之后在做VarianceThreshold会删除很多属性，所以要在归一化之前做"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_source = data_df[data_df.columns.values[:-1]]\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=(.9 * (1 - .9)))\n",
    "data_source = sel.fit_transform(data_df[data_df.columns.values[:-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "归一化，使用normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "PandasError",
     "evalue": "DataFrame constructor not properly called!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPandasError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-266-000014b97e21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_source_pre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata_source_pre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_source_pre\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/390672/anaconda3/envs/Zootopia3_4/lib/python3.4/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    301\u001b[0m                                          copy=False)\n\u001b[1;32m    302\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mPandasError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataFrame constructor not properly called!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPandasError\u001b[0m: DataFrame constructor not properly called!"
     ]
    }
   ],
   "source": [
    "data_source_pre = preprocessing.normalize(data_source, norm='l1')\n",
    "\n",
    "data_source_pre = pd.DataFrame(data_source_pre,columns=data_df.columns.values[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(704, 296)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.shape"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
